# koElectra ëª¨ë¸ í˜¸ì¶œ
from transformers import ElectraTokenizer
from transformers import TFElectraForSequenceClassification
from transformers import ElectraModel # baseëª¨ë¸ ë¶ˆëŸ¬ì˜¬ë•Œ í•„ìš”

import tensorflow as tf

# koElectra base ëª¨ë¸
# í•„ìš”ì‹œ ë°”ê¿”ì„œ í•™ìŠµì‹œí‚¤ë©´ ì•„ë¬´ë˜ë„ í° ëª¨ë¸ì´ë‹¤ë³´ë‹ˆê¹ ì„±ëŠ¥ì´ ì˜¬ë¼ê°€ì§€ì•Šì„ê¹Œ? ì˜ˆìƒí•©ë‹ˆë‹¤
# ë‹¨, small ëª¨ë¸ë³´ë‹¤ ë¬´ê²ê¸°ë•Œë¬¸ì— ì†ë„ë‚˜ GPU ë“± ìƒí™©ì„ ê³ ë ¤í•´ë´ì•¼í• ë“¯
'''
electra_model = ElectraModel.from_pretrained(
    "monologg/koelectra-base-v3-discriminator",
    num_labels=5,
    from_pt=True  # PyTorch â†’ TensorFlow ë³€í™˜
)
electra_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
'''

# koElectra í† í°ë‚˜ì´ì €
electra_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")
# koElectra ëª¨ë¸ ì •ì˜ (ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°)
# âœ… KoElectra ëª¨ë¸ ë¡œë“œ
electra_model = TFElectraForSequenceClassification.from_pretrained(
    "monologg/koelectra-small-v3-discriminator",
    num_labels=5,
    from_pt=True  # PyTorch â†’ TensorFlow ë³€í™˜
)


# í† í°í™” í•¨ìˆ˜ ì •ì˜
def electra_tokenize_function(texts, max_len):
    return electra_tokenizer(
        texts.tolist(),  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
        truncation=True,
        padding="max_length",
        max_length=max_len,
        return_tensors="tf",
        return_attention_mask=True
    )

def encode_tf_dataset(input_ids, attention_masks, labels):
    return tf.data.Dataset.from_tensor_slices((
        {"input_ids": input_ids, "attention_mask": attention_masks},  # ëª¨ë¸ ì…ë ¥
        tf.convert_to_tensor(labels, dtype=tf.int32)  # ì •ë‹µ ë¼ë²¨
    ))


def train_electra_ensemble(train_X, train_y, val_X, val_y, max_len):
    """
    koElectra ëª¨ë¸ì„ í™œìš©í•œ í•™ìŠµ ë° ë©”íƒ€ë°ì´í„° ìƒì„±.
    """
    # koElectraìš© Train ë°ì´í„° í† í°í™”
    train_encodings = electra_tokenize_function(train_X, max_len)
    train_input_ids = train_encodings["input_ids"]
    train_attention_masks = train_input_ids["attention_mask"]

    # koElectraìš© Validation ë°ì´í„° í† í°í™”
    val_encodings = electra_tokenize_function(val_X, max_len)
    val_input_ids = val_encodings["input_ids"]
    val_attention_masks = val_input_ids["attention_mask"]

    # Train ë°ì´í„°ì…‹ ë³€í™˜
    train_dataset = encode_tf_dataset(train_input_ids, train_attention_masks, train_y)

    # Validation ë°ì´í„°ì…‹ ë³€í™˜
    val_dataset = encode_tf_dataset(val_input_ids, val_attention_masks, val_y)

    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    BATCH_SIZE = 64
    train_dataset = train_dataset.shuffle(len(train_input_ids)).batch(BATCH_SIZE)
    val_dataset = val_dataset.batch(BATCH_SIZE)

    # âœ… ì˜µí‹°ë§ˆì´ì € ê³ ì • (ìµœì‹  ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ ì—ëŸ¬ ë°©ì§€)
    optimizer = tf.optimizers.Adam(learning_rate=2e-5)

    # âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ë©”íŠ¸ë¦­ ì„¤ì •
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    metrics = ["accuracy"]

    # âœ… ëª¨ë¸ ì»´íŒŒì¼
    electra_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

    # âœ… í•™ìŠµ ì‹œì‘ (EarlyStopping + ìµœì ì˜ ëª¨ë¸ ì €ì¥)
    EPOCHS = 100
    patience = 5  # EarlyStopping ì¡°ê±´ (5ë²ˆ ì—°ì† ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨)
    wait = 0  # EarlyStoppingì„ ìœ„í•œ ì¹´ìš´í„°
    best_val_accuracy = 0.0
    best_epoch = 0

    # ëª¨ë¸ í•™ìŠµ ë£¨í”„
    for epoch in range(EPOCHS):
        print(f"\nğŸ”¹ Epoch {epoch + 1}/{EPOCHS} ì‹œì‘...")

        # âœ… 1 epoch í•™ìŠµ ì§„í–‰
        history = electra_model.fit(train_dataset, validation_data=val_dataset, epochs=1, verbose=1)

        # âœ… í˜„ì¬ epochì˜ ê²€ì¦ ì •í™•ë„ ê°€ì ¸ì˜¤ê¸°
        current_val_accuracy = history.history["val_accuracy"][0]

        # âœ… ìµœì ì˜ ëª¨ë¸ ì—¬ë¶€ í™•ì¸
        if current_val_accuracy > best_val_accuracy:
            best_val_accuracy = current_val_accuracy
            best_epoch = epoch + 1
            wait = 0  # EarlyStopping ì¹´ìš´í„° ì´ˆê¸°í™”
            print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! Epoch {best_epoch}, val_accuracy={best_val_accuracy:.4f}")
        else:
            wait += 1
            print(f"âš ï¸ {wait}/{patience} - ê²€ì¦ ì •í™•ë„ ê°œì„  ì—†ìŒ.")

        # âœ… EarlyStopping ì¡°ê±´ ì¶©ì¡± ì‹œ í•™ìŠµ ì¤‘ë‹¨
        if wait >= patience:
            print(f"\nâ¹ï¸ EarlyStopping ë°œë™! {patience}ë²ˆ ì—°ì† ê°œì„ ë˜ì§€ ì•ŠìŒ â†’ í•™ìŠµ ì¢…ë£Œ")
            break

    # ë©”íƒ€ë°ì´í„° ìƒì„± (softmax í™•ë¥ ê°’)
    pred_train_koElectra = tf.nn.softmax(
        electra_model.predict([train_encodings['input_ids'], train_encodings['attention_mask']]).logits
    ).numpy()

    pred_val_koElectra = tf.nn.softmax(
        electra_model.predict([val_encodings['input_ids'], val_encodings['attention_mask']]).logits
    ).numpy()

    return pred_train_koElectra, pred_val_koElectra, electra_model

def build_meta_model_koElectra(input_dim=15):
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
    from tensorflow.keras import regularizers
    from tensorflow.keras.optimizers import Adam

    meta_model = Sequential()
    meta_model.add(Dense(128, activation='gelu', input_shape=(input_dim,), kernel_regularizer=regularizers.l2(0.01)))
    meta_model.add(BatchNormalization())
    meta_model.add(Dropout(0.4))
    meta_model.add(Dense(64, activation='gelu', kernel_regularizer=regularizers.l2(0.01)))
    meta_model.add(BatchNormalization())
    meta_model.add(Dropout(0.4))
    meta_model.add(Dense(32, activation='gelu', kernel_regularizer=regularizers.l2(0.01)))
    meta_model.add(BatchNormalization())
    meta_model.add(Dropout(0.4))
    meta_model.add(Dense(5, activation='softmax'))
    
    meta_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.005), metrics=['accuracy'])
    return meta_model